{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Assignment on Medical Image Processing\n",
    "\n",
    "## UE 183.630 - 2025 SoSe\n",
    "\n",
    "---\n",
    "\n",
    "### Assignment\n",
    "\n",
    "Aim of the second assignment is the implementation of a segmentation algorithm for bone contours. We will investigate image features extraction and selection methods as bases for image segmentation. Three different approaches are investigated, (1) a classifier based approach using Random Forests, (2) a Deep Learning approach using a UNet and (3) a bone contour Shape Model based approach that uses PCA resulting in a simplified version of Particle Filters.\n",
    "\n",
    "Relevant topics within this exercise:\n",
    "- Feature extraction\n",
    "- Classification and feature selection using Random Forests\n",
    "- Implementation and optimization of a cost function for segmentation\n",
    "\n",
    "\n",
    "#### Helper Functions in `helper_functions.py`\n",
    "\n",
    "- `get_data` loads all the necessary variables that you will be working on\n",
    "- `plot_shape` can be used to plot generated shapes and the given mean shape\n",
    "- `plot_convolutions` plots images along with their convolutions\n",
    "- `plot_prediction_triplets` plots triplets of images, segmentation predictions and segmentation ground truths\n",
    "- `show_feature_importance` visualizes the importance of a Random Forest's features\n",
    "- `evaluate_binary_segmentation` evaluates a binary segmentation using a Confusion Matrix, Dice Score, Precision and Recall\n",
    "- `optimize` is used to optimize parameters with a given cost function `f`\n",
    "- `plot_fitted_shapes` plots fitted shapes together with a segmentation and optionally with the ground truth landmarks\n",
    "\n",
    "`train_unet_model` trains a U-Net and returns the trained model and is contained in `training.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:32.971872Z",
     "start_time": "2025-06-02T17:02:31.332438Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_functions import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data used in this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:33.139620Z",
     "start_time": "2025-06-02T17:02:33.062964Z"
    }
   },
   "outputs": [],
   "source": [
    "images, masks, landmarks, aligned = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration\n",
    "\n",
    "### 1.1. Plot the first 5 bone images (stored in `images`) and the respective landmarks (stored in `landmarks`) on top.\n",
    "- Hint: `imshow` and `scatter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:33.448057Z",
     "start_time": "2025-06-02T17:02:33.148363Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Plot the first 5 bone images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Plot one set of landmarks together with the corresponding set of aligned landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:33.527908Z",
     "start_time": "2025-06-02T17:02:33.474681Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Plot one set of landmarks together with the corresponding set of aligned landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Extend `generate_shape` so that it allows rotation, scaling and translation of shapes according to the parameters `scaling, rotation, x_t, y_t` and test your function.\n",
    "\n",
    "- Copy your function `generate_shape` from Assignment 1.\n",
    "- Add functionality for rotation, scaling and translation (hint: rotation matrix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:33.546877Z",
     "start_time": "2025-06-02T17:02:33.543034Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_shape(b, eigen_vectors, mean_shape, scaling, rotation, x_t, y_t):\n",
    "    \"\"\"\n",
    "    Generate a 2D shape instance from PCA parameters and an affine transform.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b : ndarray\n",
    "        PCA weight coefficients (length <= n_components).\n",
    "    eigen_vectors : ndarray\n",
    "        PCA principal axes with shape (2N, n_components).\n",
    "    mean_shape : ndarray\n",
    "        Mean-shape vector of length 2N.\n",
    "    scaling : float\n",
    "        Scaling factor.\n",
    "    rotation : float\n",
    "        Rotation angle in degrees.\n",
    "    x_t : float\n",
    "        Translation offset in the x-direction.\n",
    "    y_t : float\n",
    "        Translation offset in the y-direction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_shape : ndarray\n",
    "        Transformed shape coordinates in the format [x1, ..., xN, y1, ..., yN].\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Generate the shape from the PCA parameters\n",
    "\n",
    "    # TODO Scale the shape\n",
    "\n",
    "    # TODO Calculate the rotation angle in radians\n",
    "\n",
    "    # TODO Create the transformation matrix\n",
    "    \n",
    "    # TODO Transform the shape using the transformation matrix\n",
    "\n",
    "    return new_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Test your adapted function `generate_shape`.\n",
    "\n",
    "- Perform PCA on the first 30 landmark sets in `aligned` with a reasonable number of components (required data shape: (30, 128)). You may use the function `PCA` from `sklearn`.\n",
    "- Use your group number assigned in TUWEL for parameterizing the following transformations:\n",
    "    - rotate by **90 + 2 * [group_number] degrees**\n",
    "    - translate by **90 + [group_number] pixels** in x direction and **-50 pixels** in y direction\n",
    "    - scale by a factor of **1.5**\n",
    "    - use a PCA weight vector of your choice\n",
    "- Apply the transformation and plot the shape and the mean_shape using `plot_shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:33.731027Z",
     "start_time": "2025-06-02T17:02:33.572331Z"
    }
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_shape\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = np.stack([lm.flatten() for lm in aligned[:30]])  # shape (30,128)\n",
    "\n",
    "n_comp = # TODO: choose number of components\n",
    "\n",
    "# TODO: Perform PCA on the data and get the eigen vectors, eigen values, and mean shape\n",
    "\n",
    "# PCA weight vector (zeros uses mean shape)\n",
    "b = # TODO: choose b\n",
    "\n",
    "# Affine transform parameters for testing\n",
    "scaling = # TODO: set scaling\n",
    "rotation = # TODO: set roation\n",
    "x_t, y_t = # TODO: set x and y translation\n",
    "\n",
    "# TODO: Generate the shape\n",
    "new_shape = \n",
    "\n",
    "# Plot the generated and mean shapes\n",
    "plot_shape(new_shape, mean_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction and Edge Detection via Convolutions\n",
    "\n",
    "### 2.1. Edge Detection via Convolutions\n",
    "#### (a) Kernels\n",
    "Create 2 edge detection kernels (image filters) represented as matrices (Fx, Fy) of dimension `[i,j]` where `i` and `j` are 3. We will focus on the filters called Prewitt Operator. These have the aim to identify edges in images by focusing on detecting changes in intensity levels. You should implement a horizontal and vertical edge detection filter. You may also create additional kernels for edge detection, if you want to experiment with these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:33.755718Z",
     "start_time": "2025-06-02T17:02:33.752387Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO Define PREWITT kernels for X and Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Convolution operation\n",
    "- Implement the convolution operation on your own as function `conv2d` with input parameters `image` representing a 2D image of size [m, n] and `kernel` representing a `3x3` filter kernel.\n",
    "- The output should be the convolved image of size [m,n]. For convolving the kerning at the image boundaries image padding with 0 values should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:33.773904Z",
     "start_time": "2025-06-02T17:02:33.771079Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv2d(image, kernel):\n",
    "    \"\"\"\n",
    "    Convolve a 2D image with a 3Ã—3 kernel, preserving input size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : ndarray\n",
    "        2D input image array.\n",
    "    kernel : ndarray\n",
    "        2D convolution kernel of shape (3, 3).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : ndarray\n",
    "        2D array of the same shape as the input image.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Initialize output array\n",
    "    output = \n",
    "\n",
    "    # TODO: add padding to preserve input size\n",
    "    \n",
    "    # TODO: Loop over the image and apply the kernel\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Test your convolution operation\n",
    "- Select 3 random bone images from `images` and convolve each of them with the filters you created.\n",
    "- Visualize the resuling images using `plot_convolutions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import plot_convolutions\n",
    "\n",
    "kernels = {\n",
    "    'Prewitt X': PREWITT_X,\n",
    "    'Prewitt Y': PREWITT_Y,\n",
    "}\n",
    "\n",
    "originals = # TODO: select 3 images\n",
    "convolved = []\n",
    "for i in originals:\n",
    "    conv = [conv2d(i, kernel) for kernel in kernels.values()]\n",
    "    convolved.append(conv)\n",
    "\n",
    "plot_convolutions(originals, convolved, kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Image Feature Computation\n",
    "- Implement `compute_features(image)` which returns a feature matrix of size `[nfeatures, npixels]` of an image, including the following features:\n",
    "    - Grey value of an image\n",
    "    - Gradient in x- and y-direction (using the kernels you created)\n",
    "    - Magnitude of the gradient\n",
    "    - x- and y-coordinates of the pixels (hint: `np.meshgrid`)\n",
    "- Feel free to implement and evaluate other additional features. You are free to use libraries such as `skimage` (e.g. from [features](https://scikit-image.org/docs/0.25.x/api/skimage.feature.html) or [filters](https://scikit-image.org/docs/0.25.x/api/skimage.filters.html)).\n",
    "- Visualize the features for the first image in `images`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: some features may be computationally expensive, so the results of `compute_features` can be cached. If you wish to not use this feature, you can remove the annotation (`@memory.cache`). In case you change your implementation of `compute_features`, do not forget to delete the directory `cache_dir` to clear the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:35.469952Z",
     "start_time": "2025-06-02T17:02:35.467022Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "cache_dir = os.path.join(os.getcwd(), 'cache_dir')\n",
    "memory = Memory(location=cache_dir, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:35.536497Z",
     "start_time": "2025-06-02T17:02:35.504275Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.feature import canny\n",
    "\n",
    "@memory.cache\n",
    "def compute_features(image):\n",
    "    \"\"\"\n",
    "    Extract features from a 2D grayscale image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        A 2D NumPy array of shape (H, W), representing a grayscale image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : np.ndarray\n",
    "        A 2D array of shape (F, N), where:\n",
    "        - F is the number of extracted features,\n",
    "        - N is the number of pixels (H Ã— W).\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    H, W = image.shape\n",
    "\n",
    "    # TODO: get the grayscale values\n",
    "\n",
    "    # TODO: get convolution results for X and Y\n",
    "    edge_x = \n",
    "    edge_y = \n",
    "\n",
    "    # TODO: get the magnitude values of the convolution results\n",
    "    gmag_x =\n",
    "    gmag_y = \n",
    "\n",
    "    # TODO: get X and Y coordinates of the pixels\n",
    "    x_coords =\n",
    "    y_coords = \n",
    "\n",
    "    # OPTIONAL TODO: add additional features (e.g.: canny, sobel)\n",
    "\n",
    "    features = np.vstack([\n",
    "        grey,\n",
    "        edge_x, edge_y,\n",
    "        gmag_x, gmag_y,\n",
    "        x_coords, y_coords,\n",
    "    ])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:35.798241Z",
     "start_time": "2025-06-02T17:02:35.569771Z"
    }
   },
   "outputs": [],
   "source": [
    "test_image = images[0]\n",
    "H, W = test_image.shape\n",
    "features_test = compute_features(test_image)\n",
    "\n",
    "# TODO: get titles and features for plotting\n",
    "titles = []\n",
    "feats = []\n",
    "\n",
    "fig, axes = plt.subplots(, figsize=(8, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    if idx < len(feats):\n",
    "        ax.imshow(feats[idx], cmap='gray', vmin=0, vmax=np.percentile(feats[idx], 99))\n",
    "        ax.set_title(titles[idx])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification and Feature Selection\n",
    "In this task we investigate how image features are selected to train a classifier to solve a binary classification task - the detection of edges (bone contours) in an image.\n",
    "\n",
    "The classification result provides per image pixel a label, where 1 denotes an edge  and 0 background/no edge. In total in this tasks we will compare and explore three bone contour extraction methods: (1) Random Forest, (2) a U-Net based Deep Learning framework and (3) Particle Filters using a defined training and testing set.\n",
    "\n",
    "\n",
    "### 3.1. Data Preparation\n",
    "- Split the image set of 50 images and the corresponding masks into a training set of 30 images (`I_train`) and masks (`y_train`) and a test set of 20 images (`I_test`) and corresponding masks (`y_test`).\n",
    "- Compute the features of all images and store them in `X_train` and `X_test`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:35.823788Z",
     "start_time": "2025-06-02T17:02:35.821457Z"
    }
   },
   "outputs": [],
   "source": [
    "I_train = \n",
    "y_train =\n",
    "I_test = \n",
    "y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Random Forest (RF)\n",
    "- Implement the function `train_rf` which takes a list of feature arrays and labels (masks) and trains a Random Forest classifier and returns the trained classifier.\n",
    "- Implement the function `predict_segmentation_rf` which takes a trained Random Forest classifier and an image and returns a predicted segmentation.\n",
    "\n",
    "#### (a) Define a Random Forest Classifier\n",
    "- Use the `RandomForestClassifier` from `sklearn`.\n",
    "- Get familiar with the parameters and select suitable ones for training.\n",
    "- The RF classifier should be trained on extracted image features and not the images directly. Thus, compute image features for all input images in `I_train` and `I_test` and store the resulting features in `X_train` and `X_test`.\n",
    "- **Hint**: to speed up the training process you should use all pixels of the bone contours but only a randomly sampled subset of the background pixels (equal amount of fore- (bone contours) and background (non-bone contours))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = \n",
    "X_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:36.177131Z",
     "start_time": "2025-06-02T17:02:36.023450Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:36.208787Z",
     "start_time": "2025-06-02T17:02:36.204477Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_rf(features, labels, n_trees=100, random_state=0):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier with class balancing via undersampling.\n",
    "\n",
    "    This function concatenates feature and label arrays, balances the classes\n",
    "    by undersampling the majority class (negative class), and trains a\n",
    "    Random Forest classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : list of np.ndarray\n",
    "        List of feature arrays to be horizontally stacked. Each array should\n",
    "        have shape (F, N), where F is the number of features and N is the number of samples.\n",
    "    labels : list of np.ndarray\n",
    "        List of label arrays (same shape as image masks). Non-zero values are treated as positive class.\n",
    "    n_trees : int, optional\n",
    "        Number of trees in the Random Forest (default is 100).\n",
    "    random_state : int, optional\n",
    "        Seed for reproducibility (default is 0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clf : RandomForestClassifier\n",
    "        Trained Random Forest classifier with out-of-bag (OOB) score enabled.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO get features and labels and train RandomForestClassifier\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Train the classifier\n",
    "- Use the image features  `X_train` as input to train a Random Forest classifier and the corresponding image masks `y_train` as class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:39.521736Z",
     "start_time": "2025-06-02T17:02:36.247173Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_clf = # TODO: train Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Predict segmentations\n",
    "- Implement the function `predict_segmentation_rf` which takes the Random Forest classifier as well as an image and returns the prediction from the classifier.\n",
    "- Visualize the prediction of the first test image (from `I_test`) and compare it to the corresponding target label from `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:39.547979Z",
     "start_time": "2025-06-02T17:02:39.545477Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_segmentation_rf(clf, image):\n",
    "    '''\n",
    "    clf: Trained Random Forest classifier\n",
    "    image: 2D grayscale image for segmentation prediction\n",
    "\n",
    "    returns:\n",
    "    segmentation: 2D array with predicted segmentation mask\n",
    "    '''\n",
    "    \n",
    "    # TODO Predict segmentation using the trained Random Forest classifier\n",
    "\n",
    "    return segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:39.795167Z",
     "start_time": "2025-06-02T17:02:39.574991Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 6))\n",
    "\n",
    "# TODO: visualize the predicted segmentation and the ground truth mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Predict all bone contours\n",
    "- Use the trained classifier and predict bone contours for the feature test set `X_test` and store the predictions in `y_pred_RF`.\n",
    "- Compare the bone contour predictions to the target labels `y_test` using the helper function `evaluate_binary_segmentation` to get Dice Score, Precision and Recall. What do these scores mean and what are they saying about the model's performance?\n",
    "- Qualitatively compare the images to the bone contour predictions and the ground truth using `plot_prediction_triplets`.\n",
    "- Evaluate and interpret the importance of different features using `show_feature_importance`. Make sure to pass the feature labels in the same order that you used to train the classifier.\n",
    "- Where is the model struggling? Where is it performing well?\n",
    "- Alternate the input features and compare the results - leave features in the training out and discuss the feature importance of the Random Forest classifier features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:44.442587Z",
     "start_time": "2025-06-02T17:02:39.821427Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_RF = # TODO: predict_segmentation_rf(rf_clf, I_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:44.477276Z",
     "start_time": "2025-06-02T17:02:44.473146Z"
    }
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_prediction_triplets, show_feature_importance, evaluate_binary_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:44.688558Z",
     "start_time": "2025-06-02T17:02:44.487986Z"
    }
   },
   "outputs": [],
   "source": [
    "results = evaluate_binary_segmentation(y_pred_RF, y_test)\n",
    "print(\"Dice Score:\", results['dice'])\n",
    "print(\"Precision:\", results['precision'])\n",
    "print(\"Recall:\", results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:46.382887Z",
     "start_time": "2025-06-02T17:02:44.733833Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_prediction_triplets(I_test, y_pred_RF, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:46.486812Z",
     "start_time": "2025-06-02T17:02:46.414337Z"
    }
   },
   "outputs": [],
   "source": [
    "show_feature_importance(rf_clf, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. U-Net\n",
    "In this task you will compare your results from the Random Forest segmentation to those from a U-Net. You will train two networks â€“ one with data augmentation and one without. Feel free to tweak any hyperparameters if needed.\n",
    "Then, you will evaluate the results and compare them to the Random Forest.\n",
    "\n",
    "(Network details: the given convolutional neural network (CNN) is a U-Net ([paper](https://arxiv.org/pdf/1505.04597)) combined with a ResNet-34 backbone ([paper](https://arxiv.org/pdf/1512.03385)). It is lightweight and should be able to run about 100 epochs in less than 10 minutes even when only using a CPU. Unlike a regular ResNet, the residual connections are made on upsampling, where regular U-Nets would have concatenations. The `train_unet_model` function takes in all images and masks and splits them 30-10-10 into training, validation and testing. The model with the smallest validation loss will be returned. Note that every time the model has a new \"best model\", it saves the checkpoint in the `checkpoints` directory. These checkpoints are not deleted automatically and may start to take up considerable storage space when training multiple times.)\n",
    "\n",
    "#### (a) Train two networks\n",
    " Train the networks `y_pred_UNet1` and `y_pred_UNet2`. The code is already given and when run, `train_unet_model` will plot training and validation loss after training. Compare the visualized loss-graphs with each other and discuss these. What is affecting them? Why are they different?\n",
    "\n",
    "If you want, you can tweak the learning rate and the number of epochs for training using the parameters `lr` and `n_epochs`. Should you have troubles with a cuda device or mps, simply use the `device_override` parameter to use a different device, e.g. `device_override='cpu'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:02:48.894698Z",
     "start_time": "2025-06-02T17:02:46.512720Z"
    }
   },
   "outputs": [],
   "source": "from training import train_unet_model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:04:50.550282Z",
     "start_time": "2025-06-02T17:02:48.919655Z"
    }
   },
   "outputs": [],
   "source": "y_pred_UNet1 = train_unet_model(images, masks, augment=False, lr=1e-4)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:06:57.778141Z",
     "start_time": "2025-06-02T17:04:50.619545Z"
    }
   },
   "outputs": [],
   "source": "y_pred_UNet2 = train_unet_model(images, masks, augment=True, lr=1e-4)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Interpret filters\n",
    "Instead of using a predefined set of filters (as we used in Task 2.1 for edge detection),  a convolutional neural network â€“ learns the filter weights  during training to extract image features in combination with convolution. In this task we want to analyse and visualize which image features are extracted by the first layer of one of the trained U-Net classifiers. Therefore, use the trained U-Net model's method `show_first_layer_outputs` and a test image of your choice.\n",
    "Interpret what image features the learned filters are extracting  (e.g. are these focusing on detecting  edges or shapes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:06:58.045819Z",
     "start_time": "2025-06-02T17:06:57.825723Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_UNet2.show_first_layer_outputs(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Predict contours\n",
    "- Use the trained U-Net classifiers to predict the bone contours for the image test set `I_test` and store the predictions in `y_pred_UNet1` and `y_pred_UNet2`.\n",
    "- Compare the bone contour predictions `y_pred_UNet[1,2]` to the target labels `y_test` the same way you did for the Random Forest.\n",
    "- Evaluate and interpret the impact of (not) using data augmentation in the training process qualitatively (visualizations â€“ you may use `plot_prediction_triplets`) and quantitatively (similarity metrics) for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:00.075022Z",
     "start_time": "2025-06-02T17:06:58.075379Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_UNet1 = # TODO: predict segmentation for y_pred_UNet1 on I_test\n",
    "y_pred_UNet2 = # TODO: predict segmentation for y_pred_UNet2 on I_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:01.348483Z",
     "start_time": "2025-06-02T17:07:00.103760Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_prediction_triplets(I_test, y_pred_UNet2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:01.643060Z",
     "start_time": "2025-06-02T17:07:01.395003Z"
    }
   },
   "outputs": [],
   "source": [
    "results_no_aug = evaluate_binary_segmentation(y_pred_UNet1, y_test)\n",
    "print(\"Dice Score:\", results_no_aug['dice'])\n",
    "print(\"Precision:\", results_no_aug['precision'])\n",
    "print(\"Recall:\", results_no_aug['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:01.841699Z",
     "start_time": "2025-06-02T17:07:01.649062Z"
    }
   },
   "outputs": [],
   "source": [
    "results_with_aug = evaluate_binary_segmentation(y_pred_UNet2, y_test)\n",
    "print(\"Dice Score:\", results_with_aug['dice'])\n",
    "print(\"Precision:\", results_with_aug['precision'])\n",
    "print(\"Recall:\", results_with_aug['recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Compare to Random Forest\n",
    "- Compare the performance of the U-Net to the Random Forest using the same training and testing data split for each method.\n",
    "- Which method performs better?\n",
    "- Are there advantages or disadvantages?\n",
    "- Discuss it qualitatively and quantitatively and accurately describe the experimental setup, in such a way it is reproducible.\n",
    "- Hint: you can use again the helper functions `plot_prediction_triplets` and `evaluate_binary_segmentation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:01.909857Z",
     "start_time": "2025-06-02T17:07:01.903766Z"
    }
   },
   "outputs": [],
   "source": [
    "from helper_functions import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Shape Particle Filters (PF)\n",
    "Shape particle filters are sequential Monte Carlo Methods used for solving a segmentation task. Therefore a shape model is required, which subsequently is fitted to the image to be segmented. For this task we will be using the PCA shape model of the first 30 images (created in Task 1.4 of this assignment) and create a fitting routine including the definition of a cost function used in the optimization process of the fitting. The shape model will be fitted to the output of the Random Forest segmentations.\n",
    "\n",
    "#### (a) Implement `fit_shape_model` and `cost_function`\n",
    "As the first step we formulate a function that models costs of fitting a shape to a target image. We are looking for a point in a parameter space (described by shape parameters, rotation, scaling, and translation) which describes an optimal fitted shape that segments the contours of a target object.\n",
    "Implement the function `fit_shape_model` which should take a target image segmentation (in our case a predicted segmentation by the RF classifier in `y_pred_RF`) and the parameters _p_ of the shape model as input. The implemented function should fit the shape model to the  target segmentation.\n",
    "- For running the optimization, use the helper function `optimize`, which takes a cost function (`cost_function`) and the upper and lower boundaries for the parameters to optimize.\n",
    "- Implement the cost function which returns a scalar value that describes how well the (from _p_) generated shapes fits the classification result. (The better the shape fits the classification result, the lower the returned value). Describe the implemented cost function in your report. The cost function will be used by `fit_shape_model` in order to run the optimization for fitting the shape model to the segmentation.\n",
    "\n",
    "Both `fit_shape_model` and `cost_function` will use `generate_shape` to generate shapes based on the shape parameters of the shape model and the transformation parameters used for scaling, rotation and translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:01.971230Z",
     "start_time": "2025-06-02T17:07:01.954135Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import distance_transform_edt as edt\n",
    "\n",
    "def cost_function(p, classification, eigen_vectors, mean_data):\n",
    "    \"\"\"\n",
    "    Cost function between PCA-generated shape and a classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : array_like\n",
    "        Shape + transform parameters [b..., scale, rotation, x_t, y_t]\n",
    "    classification : ndarray, shape (H, W)\n",
    "        Segmentation mask (values in [0, 1])\n",
    "    eigen_vectors : ndarray, shape (2N, d)\n",
    "        PCA shape eigenvectors\n",
    "    mean_data : ndarray, shape (2N,)\n",
    "        Mean shape (flattened)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : float\n",
    "        Fitting cost.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: get b, scale, rot, x_t, y_t from p and generate a shape using them\n",
    "\n",
    "    # TODO: calculate cost of fitting shape\n",
    "   \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:02.040332Z",
     "start_time": "2025-06-02T17:07:02.028770Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_shape_model(segmentation, eigen_vectors, eigen_values, mean_shape):\n",
    "    \"\"\"\n",
    "    Fit a PCA-based shape model to a binary segmentation mask using global optimization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segmentation : np.ndarray\n",
    "        Binary segmentation mask (2D array with shape HÃ—W).\n",
    "    eigen_vectors : np.ndarray\n",
    "        Matrix of eigenvectors from PCA, shape (2N, d), where N is number of shape points.\n",
    "    eigen_values : np.ndarray\n",
    "        1D array of eigenvalues from PCA, shape (d,).\n",
    "    mean_shape : np.ndarray\n",
    "        Mean shape vector, shape (2N,).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fitted_shape : np.ndarray\n",
    "        Optimized shape vector in pixel coordinates, shape (2N,).\n",
    "    best_params : np.ndarray\n",
    "        Optimal parameters [b_opt, scale, rotation_deg, x_translation, y_translation], shape (d + 4,).\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Define bounds for optimization\n",
    "    b_lower = \n",
    "    b_upper = \n",
    "    scale_lower, scale_upper = \n",
    "    rot_lower,   rot_upper   =\n",
    "    x_lower, x_upper = \n",
    "    y_lower, y_upper = \n",
    "\n",
    "    # TODO: concatenate the bounds for b, scale, rotation, x, and y into min_ and max_\n",
    "    min_ = \n",
    "    max_ = \n",
    "    assert min_.shape == max_.shape == (d + 4,)\n",
    "\n",
    "    def unified_cost_fn(p):\n",
    "        return cost_function(p, segmentation, eigen_vectors, mean_shape)\n",
    "\n",
    "    # optimize\n",
    "    best_params = optimize(unified_cost_fn, min_, max_, random_state=0)\n",
    "\n",
    "    # unpack optimal parameters\n",
    "    b_opt      = best_params[:d]\n",
    "    scale_opt  = best_params[d]\n",
    "    rot_opt    = best_params[d + 1]\n",
    "    x_t_opt    = best_params[d + 2]\n",
    "    y_t_opt    = best_params[d + 3]\n",
    "\n",
    "    # Reconstruct the final shape in image coordinates\n",
    "    fitted_shape = generate_shape(\n",
    "        b_opt,\n",
    "        eigen_vectors,\n",
    "        mean_shape,\n",
    "        scale_opt,\n",
    "        rot_opt,\n",
    "        x_t_opt,\n",
    "        y_t_opt\n",
    "    )\n",
    "\n",
    "    return fitted_shape, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Optimize for all test segmentations\n",
    "- Optimize this function for all segmentations in `y_pred_RF` (20). We are using a stochastic optimization approach called Differential Evolution for this purpose.\n",
    "- This method is very simple, robust and converges fast. We provide an implementation of this approach in optimize. An example to create and use a cost function for an optimization process can be found in `optimize_demo.py`. You can simply run it to get an example demonstration of the optimization.\n",
    "- Visualize your results using the helper function plot_fitted_shapes. You may also plot the ground truth landmarks using this function.\n",
    "- Describe the results. Where is the optimization struggling? What could be the underlying issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:02.217259Z",
     "start_time": "2025-06-02T17:07:02.215362Z"
    }
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_fitted_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:20.071105Z",
     "start_time": "2025-06-02T17:07:02.258315Z"
    }
   },
   "outputs": [],
   "source": [
    "fitted_shapes = #TODO: get fitted shapes for each segmentation in y_pred_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:07:20.809160Z",
     "start_time": "2025-06-02T17:07:20.107698Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_fitted_shapes(fitted_shapes, y_pred_RF, gt_landmarks=landmarks[30:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
